<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<meta name="description" content="A new Hugo blog by Albert Kuo.">

<meta name="twitter:card" content="summary">
<meta name="twitter:domain" content="/">

<meta name="twitter:image" content="/screenshot.png">
<meta name="twitter:title" property="og:title" itemprop="title name" content="Blog | Albert">
<meta name="twitter:description" property="og:description" itemprop="description" content="A new Hugo blog by Albert Kuo.">
<meta name="og:type" content="website">
<meta name="og:url" content="/">
<meta name="og:image" itemprop="image primaryImageOfPage" content="/screenshot.png">

<link rel="shortcut icon" href="/favicon.ico" id="favicon">
<link rel="stylesheet" href="/css/style.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-98438367-2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-98438367-2');
</script>

    

    
    
    
    <title>
        
        10 Tips and Tricks for Statistical Proofs
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title">10 Tips and Tricks for Statistical Proofs</div>

        
<div class="section" id="content">
    May 21, 2019 
    
    <div class="tag-container">
        
        <span class="tag">
            <a href="/tags/guide">
                guide
            </a>
        </span>
        
        <span class="tag">
            <a href="/tags/statistics">
                statistics
            </a>
        </span>
        
        <span class="tag">
            <a href="/tags/probability">
                probability
            </a>
        </span>
        
        <span class="tag">
            <a href="/tags/proofs">
                proofs
            </a>
        </span>
        
    </div>
    
    <hr/>
    


<p>I’ve been taking probability theory this year and I noticed that a lot of proofs will assume that the reader already knows some commonly used “tricks.” If you aren’t familiar with them, it can be hard to follow the proofs in the textbook,<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> let alone prove it yourself. I felt like this was happening to me a lot, so in an effort to better familiarize myself, I’ve written down some useful tips and tricks, along with explanations and/or examples.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<ol style="list-style-type: decimal">
<li><strong>Bounding the Union with a Sum</strong></li>
</ol>
<ul>
<li>You can always bound the probability of a union by the sum of the probabilities:
<span class="math display">\[
P\left(\cup_n A_n\right) \leq \sum_n P\left(A_n\right)
\]</span>
This is a very simple trick that uses the <a href="https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle">inclusion-exclusion principle</a>. When in doubt, try this! It’s not a very strong upper bound, but it may be sufficient for your needs.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Cauchy-Schwarz Inequality</strong></li>
</ol>
<ul>
<li><p>The Cauchy-Schwarz Inequality is pretty much unavoidable in math. Here are some canonical forms:
<span class="math display">\[
\begin{align}
|\langle u, v \rangle| &amp;\leq \|u\|\|v\| \\
\left|\sum_i u_i v_i\right|^2 &amp;\leq \sum_j|u_j|^2\sum_k|v_k|^2
\end{align}
\]</span>
A form you might encounter in statistics is the following:
<span class="math display">\[
|E(XY)|^2 \leq E(X^2)E(Y^2)
\]</span></p></li>
<li><p>Example: If <span class="math inline">\(X \sim Poisson(\lambda)\)</span> and <span class="math inline">\(Y \sim Poisson(2\lambda)\)</span>, find a bound on <span class="math inline">\(P(X \leq Y)\)</span> of the form <span class="math inline">\(A\exp(-c\lambda)\)</span>. First observe that <span class="math inline">\(P(X \geq Y) = P(e^{t(X-Y)}\geq 1) \leq E(e^{t(X-Y)})\)</span> by Chebyshev’s inequality. Then <span class="math inline">\(E(e^{t(X-Y)})=E(e^{tX}e^{-tY}) \leq \sqrt{E(e^{2tX})E(e^{-2tY})}\)</span> by Cauchy-Schwarz. The last equation gives us <span class="math inline">\(e^{(\lambda(e^{2t}-1)+2\lambda(e^{-(2t)}-1))/2}\)</span> with moment-generating functions. Take the derivative with respect to <span class="math inline">\(t\)</span> and plug back in to get the bound.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>Continuous Mapping Theorem</strong></li>
</ol>
<ul>
<li>If <span class="math inline">\(X_n \Rightarrow X_\infty\)</span> (converges in distribution) and <span class="math inline">\(g\)</span> is a measurable function that is continuous except on a set of measure 0, then <span class="math inline">\(g(X_n) \Rightarrow g(X_\infty)\)</span>. If <span class="math inline">\(g\)</span> is furthermore bounded, then <span class="math inline">\(E g(X_n) \rightarrow E g(X_\infty)\)</span>. The result is often something you hope is true and you just need to check that the function you’re applying is in fact continuous.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><strong>Convergence Theorems</strong></li>
</ol>
<ul>
<li><p>A “last step” in many proofs is to take the limit inside the integral:</p>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} \int X_n d\mu = \int \lim_{n \rightarrow \infty} X_n d\mu \\
\]</span></p>
<p>(An equivalent formulation is <span class="math inline">\(\lim_{n \rightarrow \infty} E(X_n) = E(\lim_{n \rightarrow \infty} X_n)\)</span> or <span class="math inline">\(E(X_n) \rightarrow E(X)\)</span>, where <span class="math inline">\(X\)</span> is the limit of <span class="math inline">\(X_n\)</span>.)</p></li>
<li><p>This is true under any of these three scenarios:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X_n \geq 0\)</span> and <span class="math inline">\(X_n \leq X_{n+1}\)</span> for all <span class="math inline">\(n\)</span> (monotone convergence)</li>
<li>If <span class="math inline">\(\lvert X_n\rvert \leq M\)</span> for some constant <span class="math inline">\(M\)</span> (bounded convergence)</li>
<li>If <span class="math inline">\(\lvert X_n\rvert \leq Y\)</span> for an integrable random variable <span class="math inline">\(Y\)</span>, i.e. <span class="math inline">\(E\lvert Y \rvert &lt; \infty\)</span> (dominated convergence)</li>
</ol></li>
<li><p>Example (dominated convergence): Consider the probability space <span class="math inline">\((\Omega, \mathcal{F}, \mu)\)</span>. Let <span class="math inline">\(X \geq 0\)</span> be an integrable random variable and define <span class="math inline">\(v(A) = \int_A X d \mu\)</span> for <span class="math inline">\(A \in \mathcal{F}\)</span>. To show that <span class="math inline">\(v\)</span> is a measure, a necessary condition is that <span class="math inline">\(v(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty v(A_i)\)</span> for disjoint <span class="math inline">\(A_i\)</span>. This is true since</p>
<p><span class="math display">\[
\begin{align*}
\sum_{i=1}^\infty v(A_i) &amp;= \lim_{n\rightarrow \infty} \sum_{i=1}^n v(A_i) \\
    &amp;=\lim_{n \rightarrow \infty} \sum_{i=1}^n \int_{A_i} Xd\mu \\
    &amp;=\lim_{n \rightarrow \infty}\sum_{i=1}^n \int X1_{A_i}d\mu\\
    &amp;= \lim_{n \rightarrow \infty} \int \sum_{i=1}^nX1_{A_i} d\mu \text{ since } A_i \text{ disjoint}\\
    &amp;= \int \lim_{n\rightarrow \infty}\sum_{i=1}^nX1_{A_i}d\mu \text{ since }\sum_{i=1}^nX1_{A_i} \leq X \text{ (dominated convergence)}\\
    &amp;= \int \sum_{i=1}^\infty X1_{A_i} d\mu\\
    &amp;= \int_{\cup_{i=1}^\infty A_i} Xd\mu \\
    &amp;= v(\cup_{i=1}^\infty A_i)
\end{align*}
\]</span></p></li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><strong>Expectation of Positive Random Variables</strong></li>
</ol>
<ul>
<li><p>If <span class="math inline">\(X\)</span> is a nonnegative random variable, then <span class="math inline">\(E(X) = \sum_{n=0}^\infty P(X \geq n)\)</span> or <span class="math inline">\(E(X)=\int P(X &gt; x)dx\)</span>. A proof is available on <a href="https://en.wikipedia.org/wiki/Expected_value#Formula_for_non-negative_random_variables">Wikipedia</a>. This is a really useful property and is often used for <a href="https://en.wikipedia.org/wiki/Stopping_time">stopping times</a>. More generally, <span class="math inline">\(E(X^p)=\int py^{p-1}P(X&gt;x)dx\)</span> for <span class="math inline">\(X \geq 0\)</span> and <span class="math inline">\(p&gt;0\)</span>.</p></li>
<li><p>Example: If <span class="math inline">\(\tau\)</span> is a stopping time wrt <span class="math inline">\(\mathcal{F}_n\)</span>, s.t. <span class="math inline">\(P(\tau \leq n+3|\mathcal{F}_n) &gt; 0.1\)</span> for every <span class="math inline">\(n\)</span>, then <span class="math inline">\(E(\tau) &lt; \infty\)</span>.</p>
<p>This can be proved by first showing that <span class="math inline">\(P(\tau &gt; 3k) \leq (1-0.1)^k\)</span>, which can be done using induction. Then since <span class="math inline">\(E(\tau) = \sum_{n=0}^\infty P(\tau \geq n)\)</span>, it follows that <span class="math inline">\(E(\tau) &lt; 3*\sum_{k=0}^\infty P(\tau&gt;3k) &lt; \infty\)</span>.</p></li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li><strong>Jensen’s Inequality</strong></li>
</ol>
<ul>
<li><p>Jensen’s inequality is something you should be able to recognize and apply right away. Intuitively, it says that if a function <span class="math inline">\(\phi\)</span> is <a href="https://en.wikipedia.org/wiki/Convex_function">convex</a>, then the average of the function values is greater than the function applied to the average.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
<span class="math display">\[
E[\phi(X)] \geq \phi(E[X])
\]</span>
One of the most common applications is with Euler’s number:
<span class="math display">\[
E\left[e^X\right] \geq e^{E(X)}
\]</span></p></li>
<li><p>Example: If <span class="math inline">\(X_n\)</span> is a <a href="https://en.wikipedia.org/wiki/Martingale_(probability_theory)">martingale</a>, then <span class="math inline">\((X_n+c)^2\)</span> is a submartingale. Let <span class="math inline">\(g(x) = (x+c)^2\)</span>, which is convex. Then by Jensen’s inequality, <span class="math inline">\(E(g(X_{n+1})|\mathcal{F}_n) \geq g(E(X_{n+1}|\mathcal{F}_n))\)</span>. Since <span class="math inline">\(X_n\)</span> is a martingale, <span class="math inline">\(g(E(X_{n+1}|\mathcal{F}_n))=g(X_n)=(X_n+c)^2\)</span>.</p></li>
</ul>
<ol start="7" style="list-style-type: decimal">
<li><strong>Markov’s (Chebyshev’s) Inequality</strong></li>
</ol>
<ul>
<li><p>Markov’s inequality is often used whenever you need to bound the probability. It states that
<span class="math display">\[
a^2P(|X| \geq a) \leq E(X^2;|X| \geq a) \leq E(X^2)
\]</span>
More generally, for <span class="math inline">\(\phi: \mathbb{R} \rightarrow \mathbb{R}\)</span>, <span class="math inline">\(\phi \geq 0\)</span>, <span class="math inline">\(A \in \mathcal{B}\)</span>, and <span class="math inline">\(i_A = \inf \{ \phi(y): y \in A\}\)</span>,
<span class="math display">\[
i_A P(X \in A) \leq E(\phi(X);x\in A) \leq E(\phi(X))
\]</span></p></li>
<li><p>Example: See the example for Cauchy-Schwarz (#2).</p></li>
</ul>
<ol start="8" style="list-style-type: decimal">
<li><strong>Slutsky’s Theorem</strong></li>
</ol>
<ul>
<li><p>This theorem states that if <span class="math inline">\(X_n \Rightarrow X\)</span> (in distribution) and <span class="math inline">\(Y_n \rightarrow c\)</span> in probability, then <span class="math inline">\(X_n + Y_n \Rightarrow X+c\)</span>, <span class="math inline">\(X_nY_n \Rightarrow cX\)</span>, and <span class="math inline">\(X_n/Y_n \Rightarrow Y/c\)</span>. It is not always easy to immediately see that you will need to use this theorem. However, you might be able to prove something close to the answer and realize that if you then apply Slutsky’s, the proof will be completed.</p></li>
<li><p>Example: Let <span class="math inline">\(X_1,X_2,…\)</span> are iid with <span class="math inline">\(E X_i = 0\)</span> and <span class="math inline">\(E X_i^2 = \sigma^2 &lt; \infty\)</span>, <span class="math inline">\(N_n\)</span> b a sequence of nonnegative integer-valued random variables, and <span class="math inline">\(a_n\)</span> a sequence of integers with <span class="math inline">\(a_n \rightarrow \infty\)</span> and <span class="math inline">\(N_n/a_n \rightarrow 1\)</span> in probability. If <span class="math inline">\(S_n = \sum_{i=1}^n X_i\)</span>, show that <span class="math inline">\(Y_n = S_{N_n}/\sigma \sqrt{a_n} \Rightarrow N(0, 1)\)</span>.</p>
<p>One way to prove this is to first show that <span class="math inline">\(Z_n = S_{a_n}/\sigma \sqrt{a_n} \Rightarrow N(0,1)\)</span>, which is easy to do. Then if you can show that <span class="math inline">\(Y_n - Z_n \rightarrow 0\)</span> in probability, which is harder to do, you can conclude with Slutsky’s Theorem to say that <span class="math inline">\(Y_n = (Y_n - Z_n) + Z_n \Rightarrow N(0,1)\)</span>.</p></li>
</ul>
<ol start="9" style="list-style-type: decimal">
<li><strong>Taylor Expansion &amp; Euler’s Number</strong></li>
</ol>
<ul>
<li><p>The Taylor series is often used as an approximation and bound. In particular, the Taylor series for Euler’s number <span class="math inline">\(e\)</span> is very important:
<span class="math display">\[
e^x = 1+x+\frac{x^2}{2!}+\frac{x^3}{3!} + \cdots 
\]</span></p></li>
<li><p>Another useful property of <span class="math inline">\(e\)</span> is that if <span class="math inline">\(a_n \rightarrow a\)</span>, then
<span class="math display">\[
e^a = \lim_{n \rightarrow \infty}\left(1+\frac{a_n}{n}\right)^n
\]</span></p></li>
<li><p>Example: See the example for characteristic functions (Bonus #2).</p></li>
</ul>
<ol start="10" style="list-style-type: decimal">
<li><strong>Truncating an Infinite Sum</strong></li>
</ol>
<ul>
<li><p>To get bounds on an infinite sum, you will generally want to truncate it
<span class="math display">\[
\sum_{n=1}^\infty = \sum_{n=1}^N+\sum_{n &gt; N}
\]</span>
The idea is that you can bound the two sums on the right individually. First, you can bound <span class="math inline">\(\sum_{n=1}^N\)</span> because the individual summands are bounded by some <span class="math inline">\(\epsilon\)</span>, so the whole sum is bounded by <span class="math inline">\(\epsilon N\)</span>. Second, you can bound the rest of the sum <span class="math inline">\(\sum_{n&gt;N}\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span>.</p></li>
<li><p>Example: Let <span class="math inline">\(\mu_n, \mu\)</span> be probability measures on <span class="math inline">\(\mathbb{N}\)</span>. Show that if <span class="math inline">\(\mu_n(x) \rightarrow \mu(x)\)</span> for each <span class="math inline">\(x \in \mathbb{N}\)</span>, then the total variation distance <span class="math inline">\(d(\mu_n,\mu) = \frac{1}{2}\sum_{x=1}^\infty |\mu_n(x)-\mu(x)| \rightarrow 0\)</span>.</p>
<p>If we just tried to bound the infinite sum directly using the assumption, we would get a bound of <span class="math inline">\(\epsilon*\infty\)</span>. Therefore, we need to split the infinite sum.
<span class="math display">\[
\begin{align}
\frac{1}{2}\sum_{x=1}^\infty |\mu_n(x)-\mu(x)| &amp;\leq \frac{1}{2}\sum_{x=1}^N|\mu_n(x)-\mu(x)| + \frac{1}{2}\sum_{x=N}^\infty |\mu_n(x)-\mu(x)|\\
&amp;= A_n+B_n\\
\end{align}
\]</span>
Now we can bound the finite sum “<span class="math inline">\(A_n\)</span>” and the rest of the sum “<span class="math inline">\(B_n\)</span>” separately.</p>
<ol style="list-style-type: decimal">
<li>Since <span class="math inline">\(\mu_n(x)\rightarrow \mu(x)\)</span>, bound <span class="math inline">\(|\mu_n(x) - \mu(x)| &lt; \epsilon_1\)</span> for <span class="math inline">\(n \geq N_1\)</span>. Thus, <span class="math inline">\(A_n &lt; \epsilon_1N\)</span>.</li>
<li>Since <span class="math inline">\(\mu_n,\mu\)</span> are probability measures, then both <span class="math inline">\(P(|X_n|&gt;N)\)</span> and <span class="math inline">\(P(|X|&gt;N) \rightarrow 0\)</span> as <span class="math inline">\(N\rightarrow \infty\)</span>, where <span class="math inline">\(X_n\)</span> has distribution <span class="math inline">\(\mu_n\)</span> and <span class="math inline">\(X\)</span> has distribution <span class="math inline">\(\mu\)</span>. In other words, <span class="math inline">\(\sum_{z=N}^\infty \mu_n(z) &lt; \epsilon_2\)</span> and <span class="math inline">\(\sum_{z=N}^\infty \mu(z) &lt; \epsilon_2\)</span> for <span class="math inline">\(N\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> Then <span class="math inline">\(\sum_{z=N_2}^\infty |\mu_n(x) - \mu(x)| &lt; 2\epsilon_2\)</span>. Thus, <span class="math inline">\(B_n &lt; \epsilon_2\)</span>.</li>
</ol>
<p>To conclude, we have shown that <span class="math inline">\(A_n+B_n &lt; \epsilon_1N+\epsilon_2 \rightarrow 0\)</span>. Since <span class="math inline">\(N\)</span> is finite and we can make <span class="math inline">\(\epsilon_1, \epsilon_2\)</span> arbitrarily small, then <span class="math inline">\(A_n+B_n \rightarrow 0\)</span>.</p></li>
</ul>
<div id="bonus" class="section level2">
<h2>Bonus</h2>
<p>Here are two additional useful techniques. I’ve listed them separately because it seems a bit of a stretch to call them “tricks.”<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<ol style="list-style-type: decimal">
<li><strong>Borel-Cantelli Lemmas</strong></li>
</ol>
<ul>
<li>The first Borel-Cantelli Lemma states that if <span class="math inline">\(\sum_{n=1}^\infty P(A_n) &lt; \infty\)</span>, then <span class="math inline">\(P(A_n \text{ i. o.})= 0\)</span>. This is often used to prove almost sure convergence. For example, if you can show that <span class="math inline">\(\sum_{n=1}^\infty P(|X_n-X|&gt;\epsilon)&lt;\infty\)</span>, then by the lemma, <span class="math inline">\(P(|X_n-X|&gt;\epsilon \text{ i.o.}) = 0\)</span>, which implies that <span class="math inline">\(X_n \rightarrow X\)</span> almost surely by definition.</li>
<li>The second Borel-Cantelli Lemma states that if <span class="math inline">\(A_n\)</span> are mutually independent and <span class="math inline">\(\sum_{n=1}^\infty P(A_n) = \infty\)</span>, then <span class="math inline">\(P(A_n \text{ i.o.})= 1\)</span>. Similarly, this can be used to prove that <span class="math inline">\(X_n\)</span> doesn’t converge to <span class="math inline">\(X\)</span> almost surely. It’s a little less useful than B-C 1 because of the extra requirement of independence and the fact that you usually want to prove something converges rather than doesn’t converge.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Characteristic Functions</strong></li>
</ol>
<ul>
<li><p>There is a one to one correspondence between a characteristic function <span class="math inline">\(\phi(t)\)</span> and a distribution, so it’s sometimes useful to use characteristic functions to prove something about a distribution. By definition, the characteristic function is
<span class="math display">\[
\phi(t) = Ee^{itx} = E \cos tX + iE\sin tX
\]</span>
Note that it is a function of <span class="math inline">\(t\)</span> and that it lies within the unit circle <span class="math inline">\(|\phi(t)| \leq E|e^{itX}| = 1\)</span>.</p></li>
<li><p>Example: We can prove the central limit theorem with characteristic functions. Assume that <span class="math inline">\(X_1,X_2,…\)</span> are iid with <span class="math inline">\(E X_i = 0\)</span> and <span class="math inline">\(var(X_i) = \sigma^2 &lt; \infty\)</span>. The central limit theorem states that if <span class="math inline">\(S_n = \sum X_i\)</span>,
<span class="math display">\[
\frac{S_n}{\sigma n^{1/2}} \Rightarrow \chi \sim N(0, 1)
\]</span>
To show this, note that by Taylor expansion<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>, the characteristic function of <span class="math inline">\(X_i\)</span> is
<span class="math display">\[
\begin{align}
\phi(t) = E \exp(itX_i) &amp;= 1 + it EX - \frac{t^2E(X^2)}{2} + o(t^2) \\
\implies E \exp\left(\frac{itS_n}{\sigma n^{1/2}}\right) &amp;= \left[E \exp \frac{itX_i}{\sigma n^{1/2}}\right]^n \\
&amp;= \left[1-\frac{t^2}{2n}+\sigma(n^{-1})\right]^n \\
\end{align}
\]</span>
By property of Euler’s number (see #9), the last term <span class="math inline">\(\rightarrow \exp\left(\frac{-t^2}{2}\right)\)</span>, which is the characteristic function of a standard normal distribution.</p></li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Specifically, Durrett’s <a href="https://services.math.duke.edu/~rtd/PTE/pte.html">Probability Theory and Examples</a> textbook.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This guide was also inspired by Daniel Seita’s <a href="https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics">Mathematical Tricks Commonly Used in Machine Learning and Statistics</a>.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>For more intuitive explanations, check out this <a href="https://www.quora.com/What-is-an-intuitive-explanation-of-Jensens-Inequality">post on quora</a>.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>To be formal, you can use separate <span class="math inline">\(N\)</span>’s to bound <span class="math inline">\(\mu_n\)</span> and <span class="math inline">\(\mu\)</span> and then use the max as your <span class="math inline">\(N\)</span>.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>And also because “12 Tips and Tricks” is somewhat less catchy than “10 Tips and Tricks.”<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>Technically, you have to assume <span class="math inline">\(E|X|^3 &lt; \infty\)</span> for a direct application of the Taylor expansion, but with some tedious math, it can be shown that <span class="math inline">\(E|X|^2 &lt; \infty\)</span> is sufficient.<a href="#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>

</div>


        
<div class="section bottom-menu">
    
<hr />
<p>


    
        <a class="menu" href="/post">back</a>
        
            &#183;
        
    

    
        
            <a class="menu" href="/new">
                new? start here
            </a>
        
    
    
        
            &#183; 
            <a class="menu" href="/post">
                posts
            </a>
        
            &#183; 
            <a class="menu" href="/about">
                about
            </a>
        
    
    &#183; 
    <a class="menu" href="/">
        main
    </a>

</p>

</div>


        <div class="section footer">
<script src="//yihui.name/js/math-code.js"></script>
<script async
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-98438367-2', 'auto');
	
	ga('send', 'pageview');
}
</script>



<link rel="stylesheet" href="/css/github.css" rel="stylesheet" id="theme-stylesheet">
<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


</div>
    </div>
</body>

</html>